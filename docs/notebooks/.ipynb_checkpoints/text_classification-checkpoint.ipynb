{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540e7f49",
   "metadata": {},
   "source": [
    "# 数据集介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ef266",
   "metadata": {},
   "source": [
    "数据集类型是微博情感分类 \n",
    "\n",
    "来源https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/weibo_senti_100k/intro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5e690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d3cdb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/root/NLP_warehouse/nlp-basictasks/docs/notebooks', '/root/miniconda3/lib/python38.zip', '/root/miniconda3/lib/python3.8', '/root/miniconda3/lib/python3.8/lib-dynload', '', '/root/.local/lib/python3.8/site-packages', '/root/miniconda3/lib/python3.8/site-packages', '/root/miniconda3/lib/python3.8/site-packages/IPython/extensions', '/root/.ipython', '/root/NLP_warehouse/reconstruct/basictask']\n"
     ]
    }
   ],
   "source": [
    "module_path=os.path.abspath('/root/NLP_warehouse/reconstruct/basictask/')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ffd5e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tasks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dd750f1386ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtasks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclsEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tasks'"
     ]
    }
   ],
   "source": [
    "from tasks import cls\n",
    "from evaluation import clsEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d79288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from nlp_basictasks.tasks import cls\n",
    "from nlp_basictasks.evaluation import clsEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cebfab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论数目（总体）：119988\n",
      "评论数目（正向）：59993\n",
      "评论数目（负向）：59995\n"
     ]
    }
   ],
   "source": [
    "data_path='/root/datasets/weibo_senti_100k.csv'\n",
    "pd_all = pd.read_csv(data_path)\n",
    "\n",
    "print('评论数目（总体）：%d' % pd_all.shape[0])\n",
    "print('评论数目（正向）：%d' % pd_all[pd_all.label==1].shape[0])\n",
    "print('评论数目（负向）：%d' % pd_all[pd_all.label==0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc368d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66555</th>\n",
       "      <td>0</td>\n",
       "      <td>@CorrineCz 粉色太可爱了！ //@M甄琪:回复@库拉钩儿:这么应景儿但主角不在，真...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107971</th>\n",
       "      <td>0</td>\n",
       "      <td>上海挺冷，穿的能hold住我在这里，杯具的是没有大头来充伞啊！这下雨天啊啊啊啊[抓狂]#浦东...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87919</th>\n",
       "      <td>0</td>\n",
       "      <td>真的假的[衰]//@刘上_座驾car:够河蟹，太邪恶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86690</th>\n",
       "      <td>0</td>\n",
       "      <td>[可怜]木有人呀 [抓狂]//@Shine_帅: 你打算对谁表白呢?[亲亲] //@Sunf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108232</th>\n",
       "      <td>0</td>\n",
       "      <td>【#金基德#《莫比乌斯》：专注毁三观】金导演多年致力于讨论伦理、性等问题，新片离奇程度已不能...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50697</th>\n",
       "      <td>1</td>\n",
       "      <td>不是模仿，是嘲讽。[嘻嘻]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86245</th>\n",
       "      <td>0</td>\n",
       "      <td>不用都不知道自己脸这么脏，用完脸白了三度！[吃惊]按摩3~5分钟然后用纸巾或化妆棉擦下来，如...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14065</th>\n",
       "      <td>1</td>\n",
       "      <td>金桥的同学有福啦。值得推广 //@川食公馆长空哥:是的！这是真的！[嘻嘻][哈哈][馋嘴][馋嘴]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40831</th>\n",
       "      <td>1</td>\n",
       "      <td>吃得一往情深啊！[哈哈] 其实快乐无处不在…@黄天space @BH-judy @clove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3639</th>\n",
       "      <td>1</td>\n",
       "      <td>喝茶，走起~~[太开心]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73222</th>\n",
       "      <td>0</td>\n",
       "      <td>[委屈][可怜][失望] //@民建张皎:[蜡烛][蜡烛][蜡烛]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31115</th>\n",
       "      <td>1</td>\n",
       "      <td>high后畅聊，顺风回家[爱你][爱你] //@巨匠娱乐:#人生就是一场运动会#12.24来...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99152</th>\n",
       "      <td>0</td>\n",
       "      <td>回复@rain绛珠草:[怒] //@rain绛珠草:查查是不是鸡瘟吧</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117769</th>\n",
       "      <td>0</td>\n",
       "      <td>自戳双目啊[泪]//@张伟_Zor: “只有灵魂的丰富与华美已无需外在弥补之后，人才能坦然发...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8685</th>\n",
       "      <td>1</td>\n",
       "      <td>嘻哈包袱铺四周年庆典：欢乐嘉年华：[哈哈]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50957</th>\n",
       "      <td>1</td>\n",
       "      <td>我怎么那么善于发现错误啊！！庆幸我发现了[嘻嘻][嘻嘻]！否则后来的日子就难过了！[酷][酷]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115962</th>\n",
       "      <td>0</td>\n",
       "      <td>#摄友点评# 三年以前，那时候我有了单反，嘻嘻，拍了老家的喵族。。。希望时间能够倒退，退到想...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101484</th>\n",
       "      <td>0</td>\n",
       "      <td>[泪]俺在公车上，要坐31个站，好不容易过了10个站，泰和花园站上来一漂亮时尚美眉坐我旁边，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80997</th>\n",
       "      <td>0</td>\n",
       "      <td>咱们这点出息[委屈][偷笑] //@二狗ta妈: 我明天想去吃榴莲鸡！[泪]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102620</th>\n",
       "      <td>0</td>\n",
       "      <td>在国际饭店背后主题餐厅，一层都是蒙古包，吃各种羊，喝各种酒。。。[晕]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                             review\n",
       "66555       0  @CorrineCz 粉色太可爱了！ //@M甄琪:回复@库拉钩儿:这么应景儿但主角不在，真...\n",
       "107971      0  上海挺冷，穿的能hold住我在这里，杯具的是没有大头来充伞啊！这下雨天啊啊啊啊[抓狂]#浦东...\n",
       "87919       0                         真的假的[衰]//@刘上_座驾car:够河蟹，太邪恶\n",
       "86690       0  [可怜]木有人呀 [抓狂]//@Shine_帅: 你打算对谁表白呢?[亲亲] //@Sunf...\n",
       "108232      0  【#金基德#《莫比乌斯》：专注毁三观】金导演多年致力于讨论伦理、性等问题，新片离奇程度已不能...\n",
       "50697       1                                      不是模仿，是嘲讽。[嘻嘻]\n",
       "86245       0  不用都不知道自己脸这么脏，用完脸白了三度！[吃惊]按摩3~5分钟然后用纸巾或化妆棉擦下来，如...\n",
       "14065       1  金桥的同学有福啦。值得推广 //@川食公馆长空哥:是的！这是真的！[嘻嘻][哈哈][馋嘴][馋嘴]\n",
       "40831       1  吃得一往情深啊！[哈哈] 其实快乐无处不在…@黄天space @BH-judy @clove...\n",
       "3639        1                                       喝茶，走起~~[太开心]\n",
       "73222       0                  [委屈][可怜][失望] //@民建张皎:[蜡烛][蜡烛][蜡烛]\n",
       "31115       1  high后畅聊，顺风回家[爱你][爱你] //@巨匠娱乐:#人生就是一场运动会#12.24来...\n",
       "99152       0                 回复@rain绛珠草:[怒] //@rain绛珠草:查查是不是鸡瘟吧\n",
       "117769      0  自戳双目啊[泪]//@张伟_Zor: “只有灵魂的丰富与华美已无需外在弥补之后，人才能坦然发...\n",
       "8685        1                              嘻哈包袱铺四周年庆典：欢乐嘉年华：[哈哈]\n",
       "50957       1    我怎么那么善于发现错误啊！！庆幸我发现了[嘻嘻][嘻嘻]！否则后来的日子就难过了！[酷][酷]\n",
       "115962      0  #摄友点评# 三年以前，那时候我有了单反，嘻嘻，拍了老家的喵族。。。希望时间能够倒退，退到想...\n",
       "101484      0  [泪]俺在公车上，要坐31个站，好不容易过了10个站，泰和花园站上来一漂亮时尚美眉坐我旁边，...\n",
       "80997       0             咱们这点出息[委屈][偷笑] //@二狗ta妈: 我明天想去吃榴莲鸡！[泪]\n",
       "102620      0                在国际饭店背后主题餐厅，一层都是蒙古包，吃各种羊，喝各种酒。。。[晕]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_all.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94faee42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119988\n"
     ]
    }
   ],
   "source": [
    "from readers.cls import getExamplesFromData\n",
    "print(len(pd_all))\n",
    "random_idx=np.random.permutation(len(pd_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e976976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 20000\n"
     ]
    }
   ],
   "source": [
    "sentences=pd_all['review'].values[random_idx].tolist()[:20000]\n",
    "labels=pd_all['label'].values[random_idx].tolist()[:20000]\n",
    "print(len(sentences),len(labels))\n",
    "random_idx=np.random.permutation(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a559f866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 128 : *****************************Logging some train examples*****************************\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 129 : Total train nums is : 16000\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 132 : \n",
      "sidras不错不错～！//@西班牙美食美酒: 天气渐热，清爽苹果酒、桑格利亚果酒喝起来！！！[鼓掌]\t1\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 132 : \n",
      "高考真的把人逼疯了，这个质问很好！！！！[怒]//@三号院切糕: 转发微博\t0\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 132 : \n",
      "体育评论员总是这样说：＂。。。他已经是一名久经沙场的老队员了，生于1989年的他是怎样怎样。。。＂[怒][怒][怒]完全不在乎普通观众的感受！\t0\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 132 : \n",
      "//@薇薇情感语录: [晕]\t0\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 132 : \n",
      "@嘻哈当家主持小宝 学学人家～//@围脖经典语录：情何以堪…[抓狂]\t0\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 128 : *****************************Logging some dev examples*****************************\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 129 : Total dev nums is : 4000\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 132 : \n",
      "回复@小学生Ivan:广东省博物馆！珠江珠城！[哈哈][嘻嘻] //@小学生Ivan:在哪阿，糖哥[good] //@糖哥-咖啡大佬老冯:回复@柯鹏-海德堡:谢谢！  //@柯鹏-海德堡:致以热烈的祝贺！\t1\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 132 : \n",
      "林小姐的故事同样难过。6点就起床收拾行李去机场，结果生生在飞机上坐了一个半小时才飞。到酒店了，突然心跳加快、头晕恶心。还想起昨晚跟你讨论的话题好恐怖啊@王檀冰Sherry//@焦中华eliza:焦小姐的故事也很难过，想早点回北京讨论完善ppt，飞机至少推迟近六个多小时，不知道今天能不能回到故乡[泪]\t0\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 132 : \n",
      "[衰]//@万娜: //@chicat: [哈哈]嗯嗯 //@Tian小猫: 推荐点人过来呗，@文如破竹 @chicat 都要人；产品经理也要 ，发给我[哈哈] //@邱志东:一声长叹。\t1\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 132 : \n",
      "她来了哦～，大年初一早上九点中央三套@CCTV文化视点 不见不散哦～[嘻嘻]@梅青木? @泉全之心\t1\n",
      "2021-08-21 16:42:39 - INFO - getExamplesFromData - 132 : \n",
      "又醒了，生病的日子真煎熬，每天睡不好白天还得象女斗士地玩命工作[泪]\t0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "label2id={'0':0,'1':1}\n",
    "dev_ratio=0.2\n",
    "dev_nums=int(len(sentences)*dev_ratio)\n",
    "train_nums=len(sentences)-dev_nums\n",
    "print(dev_nums)\n",
    "train_sentences=sentences[:train_nums]\n",
    "train_labels=labels[:train_nums]\n",
    "dev_sentences=sentences[-dev_nums:]\n",
    "dev_labels=labels[-dev_nums:]\n",
    "train_examples,max_seq_len=getExamplesFromData(sentences=train_sentences,labels=train_labels,label2id=label2id,mode='train',return_max_len=True)\n",
    "dev_examples=getExamplesFromData(sentences=dev_sentences,labels=dev_labels,label2id=label2id,mode='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed96a5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 1998\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_labels),sum(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e279b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 16:42:39 - INFO - __init__ - 48 : The label2id is\n",
      " {\"0\": 0, \"1\": 1}\n",
      "2021-08-21 16:42:39 - INFO - __init__ - 58 : Loading model from /data/nfs14/nfs/aisearch/asr/xhsun/CommonModel/chinese-roberta-wwm/, which is from huggingface model\n",
      "2021-08-21 16:42:39 - INFO - get_config_dict - 177 : loading configuration file /data/nfs14/nfs/aisearch/asr/xhsun/CommonModel/chinese-roberta-wwm/config.json\n",
      "2021-08-21 16:42:39 - INFO - from_pretrained - 404 : loading bert model file /data/nfs14/nfs/aisearch/asr/xhsun/CommonModel/chinese-roberta-wwm/\n",
      "2021-08-21 16:42:39 - INFO - from_pretrained - 423 : BertConfig has been loaded from /data/nfs14/nfs/aisearch/asr/xhsun/CommonModel/chinese-roberta-wwm/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 16:42:41 - INFO - from_pretrained - 125 : loading vocabulary file /data/nfs14/nfs/aisearch/asr/xhsun/CommonModel/chinese-roberta-wwm/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "model_path='/data/nfs14/nfs/aisearch/asr/xhsun/CommonModel/chinese-roberta-wwm/'\n",
    "print(max_seq_len)\n",
    "max_seq_len=min(512,max_seq_len)\n",
    "cls_model=cls(model_path=model_path,label2id=label2id,max_seq_length=max_seq_len,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "603f5de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size=32\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87cc9f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nlp_basictasks.tasks.cls.cls at 0x7f8d22580f70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2773784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0, '1': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "943fd154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 16:42:41 - INFO - __init__ - 28 : label2id like : {'0': 0, '1': 1}\n",
      "2021-08-21 16:42:41 - INFO - __init__ - 39 : The number of 0 in dataset is 2002\n",
      "2021-08-21 16:42:41 - INFO - __init__ - 39 : The number of 1 in dataset is 1998\n",
      "2021-08-21 16:42:41 - INFO - __init__ - 45 : Evalautor sentence like : \n",
      "\n",
      "2021-08-21 16:42:41 - INFO - __init__ - 47 : 祝全家游开心顺利[哈哈][good]\t1\n",
      "\n",
      "2021-08-21 16:42:41 - INFO - __init__ - 47 : [哈哈][哈哈]//@李国斌律师:原来以为三娃是一个脱离了低级趣味的人，不想到总看到这娃逗狗狗玩。无聊得很[鄙视]//@文三娃: 下午去玉渊潭踏青，居然第一时间错过了这首好诗！郭老对不起，我来晚了！[赞]\t1\n",
      "\n",
      "2021-08-21 16:42:41 - INFO - __init__ - 47 : 哇哦！我已经开始期待啦[太开心]\t1\n",
      "\n",
      "2021-08-21 16:42:41 - INFO - __init__ - 47 : [抓狂]早早托人买好了十一去苏州的车票，可迟迟买不到回程的车票，就不明白了，为什么火车站和售票点永远都没有车票，而人家票贩子那里就有。\t0\n",
      "\n",
      "2021-08-21 16:42:41 - INFO - __init__ - 47 : 早知道好好练一下台球就不用眼神空洞了，but 桌上足球真是个锻炼腹肌的游戏啊，笑惨了[偷笑]圆润臀部还行，不能嘲笑国际友人哈[嘻嘻]//@Chris_Tang: 嗯，某人眼神空洞意外出镜了，郑重声明那个圆润的臀部可不是我啊！@glassrose\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator=clsEvaluator(sentences=dev_sentences,label_ids=dev_labels,write_csv=False,label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bdede3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 16:42:41 - INFO - fit - 101 : 当前是单句子分类任务\n",
      "2021-08-21 16:42:41 - INFO - fit - 112 : 一个epoch 下，每隔100个step会输出一次loss，每隔250个step会评估一次模型\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40db860411484a6a86f0777c4327d208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41713f5e22fc47d19e27a3d82f2f434b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 16:42:45 - INFO - __call__ - 63 : ClsEvaluator: Evaluating the model on  dataset in epoch 0 after 1 steps:\n",
      "2021-08-21 16:42:45 - INFO - predict - 220 : 当前是单句子分类任务预测\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc8236254a74a30918fd7b09744c8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 16:44:33 - INFO - __call__ - 70 : Accuracy: 0.492\n",
      "2021-08-21 16:44:33 - INFO - __call__ - 82 : 0\tprecision : {} recall : {} f1 score : 0.4615262194636747\n",
      "2021-08-21 16:44:33 - INFO - __call__ - 82 : 1\tprecision : {} recall : {} f1 score : 0.49544439568204335\n",
      "2021-08-21 16:44:33 - INFO - __call__ - 90 : AUC: 0.513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 16:44:35 - INFO - save_pretrained - 509 : Model weights saved in /data/nfs14/nfs/aisearch/asr/xhsun/tmp_model/BERT/pytorch_model.bin\n",
      "2021-08-21 16:44:35 - INFO - save_pretrained - 150 : Configuration saved in /data/nfs14/nfs/aisearch/asr/xhsun/tmp_model/BERT/config.json\n",
      "2021-08-21 16:44:35 - INFO - save_vocab - 51 : Vocab saved in /data/nfs14/nfs/aisearch/asr/xhsun/tmp_model/BERT/vocab.txt\n",
      "2021-08-21 16:44:35 - INFO - fit - 187 : In epoch 0, training_step 0, the eval score is 0.5131790131790132, previous eval score is -9999999, model has been saved in /data/nfs14/nfs/aisearch/asr/xhsun/tmp_model/\n",
      "2021-08-21 16:49:47 - INFO - fit - 165 : Epoch : 0, train_step : 100/500, loss_value : 0.3366504054144025 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5652511f0b8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcls_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/data/nfs14/nfs/aisearch/asr/xhsun/tmp_model/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/nlp_basictasks/tasks/cls.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, is_pairs, train_dataloader, evaluator, epochs, loss_fct, scheduler, warmup_proportion, optimizer_type, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, early_stop_patience, print_loss_step, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0mskip_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mscale_before_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                     \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                     \u001b[0mloss_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#CrossEntropyLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0mtraining_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/nlp_basictasks/heads/cls.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, label_ids, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mlabel_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         '''\n\u001b[0;32m---> 97\u001b[0;31m         (sequence_outputs,pooled_output)=self.bert(input_ids=input_ids,\n\u001b[0m\u001b[1;32m     98\u001b[0m                                                   \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                                                    \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/nlp_basictasks/modules/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         encoded_layers = self.encoder(embedding_output,\n\u001b[0m\u001b[1;32m    586\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                                       output_all_encoded_layers=output_all_encoded_layers)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/nlp_basictasks/modules/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/nlp_basictasks/modules/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/nlp_basictasks/modules/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mself_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/nlp_basictasks/modules/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cls_model.fit(is_pairs=False,train_dataloader=train_dataloader,evaluator=evaluator,output_path=\"/data/nfs14/nfs/aisearch/asr/xhsun/tmp_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bbbe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "really_import=cls(model_path='/data/nfs14/nfs/aisearch/asr/xhsun/tmp_model',\n",
    "                  label2id=label2id,max_seq_length=max_seq_len,device='cpu',is_finetune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4083534",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=really_import.predict(is_pairs=False,dataloader=dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73fc604c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.981"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((np.argmax(predicts,axis=1)==np.array(dev_labels)).tolist())/len(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5298f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6929bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    def __init__(self, num_tags, batch_first=True, hasTraining=False) -> None:\n",
    "        super().__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        if hasTraining==False:\n",
    "            self.start_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "            self.end_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "            self.transitions = nn.Parameter(torch.empty(num_tags,num_tags))\n",
    "            self.init_matrix()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def init_matrix(self):\n",
    "        nn.init.uniform_(self.start_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.end_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c40eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf=CRF(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85e0a05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sds/dsd/ad'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(\"sds\",'dsd','ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed37f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obj=crf.transitions,f='./tmp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "539f802f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0763,  0.0966, -0.0893, -0.0571, -0.0983, -0.0413,  0.0384,  0.0052,\n",
       "          0.0677,  0.0197,  0.0469,  0.0046,  0.0783, -0.0790, -0.0673, -0.0206,\n",
       "         -0.0765, -0.0227,  0.0540, -0.0875],\n",
       "        [ 0.0077,  0.0380, -0.0522, -0.0449, -0.0687,  0.0733, -0.0563,  0.0222,\n",
       "         -0.0178, -0.0719,  0.0617, -0.0544,  0.0430,  0.0310, -0.0394, -0.0492,\n",
       "         -0.0650,  0.0193, -0.0395,  0.0256],\n",
       "        [-0.0288,  0.0377,  0.0286, -0.0850,  0.0333, -0.0450,  0.0586, -0.0283,\n",
       "         -0.0705, -0.0768, -0.0528,  0.0722, -0.0189,  0.0898,  0.0761, -0.0595,\n",
       "          0.0618,  0.0099,  0.0679,  0.0191],\n",
       "        [-0.0255, -0.0240, -0.0407,  0.0783,  0.0833,  0.0522,  0.0329, -0.0547,\n",
       "          0.0404,  0.0451,  0.0487, -0.0752, -0.0327, -0.0162, -0.0277, -0.0892,\n",
       "         -0.0172, -0.0717,  0.0907,  0.0358],\n",
       "        [-0.0634,  0.0128, -0.0769,  0.0997, -0.0183, -0.0625, -0.0745, -0.0118,\n",
       "          0.0705, -0.0982, -0.0611,  0.0490,  0.0317, -0.0486, -0.0417,  0.0550,\n",
       "          0.0839,  0.0384,  0.0402,  0.0790],\n",
       "        [-0.0486,  0.0673,  0.0849, -0.0361,  0.0567, -0.0731,  0.0538, -0.0178,\n",
       "         -0.0341,  0.0047,  0.0042, -0.0108,  0.0559, -0.0152, -0.0189, -0.0575,\n",
       "         -0.0087,  0.0078,  0.0327, -0.0684],\n",
       "        [-0.0159,  0.0467,  0.0257, -0.0955,  0.0575, -0.0840, -0.0289, -0.0315,\n",
       "          0.0163,  0.0988,  0.0474,  0.0753,  0.0835, -0.0714, -0.0879,  0.0417,\n",
       "          0.0558, -0.0340,  0.0243, -0.0287],\n",
       "        [ 0.0078,  0.0793, -0.0919,  0.0028, -0.0751,  0.0540, -0.0990, -0.0797,\n",
       "          0.0918,  0.0987,  0.0108, -0.0810, -0.0562, -0.0471, -0.0617,  0.0470,\n",
       "          0.0229,  0.0105, -0.0495,  0.0161],\n",
       "        [ 0.0715, -0.0283,  0.0087,  0.0568, -0.0100,  0.0296,  0.0057, -0.0349,\n",
       "          0.0823,  0.0490,  0.0883, -0.0211,  0.0981,  0.0081, -0.0153, -0.0751,\n",
       "         -0.0789,  0.0819,  0.0951, -0.0904],\n",
       "        [-0.0402,  0.0627, -0.0187,  0.0255, -0.0653,  0.0204,  0.0643, -0.0172,\n",
       "         -0.0320,  0.0624, -0.0170,  0.0963, -0.0314, -0.0788, -0.0895, -0.0849,\n",
       "          0.0747, -0.0555,  0.0956, -0.0364]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('./tmp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c2804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea3749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce861cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
